{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "#from models import AlexNet\n",
    "from torchvision import models\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "data_dir = '../data/'\n",
    "test_dir = 'normal_test'\n",
    "classes = sorted(os.listdir(data_dir + test_dir))\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = [0.44947562, 0.46524084, 0.40037745]\n",
    "std = [0.18456618, 0.16353698, 0.20014246]\n",
    "#mean = [0.14318287, 0.19182085, 0.10939839]\n",
    "#std = [0.20657195, 0.25984347, 0.1585114]\n",
    "\n",
    "data_transforms = {\n",
    "        'test': transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)])}\n",
    "\n",
    "test_images = datasets.ImageFolder(os.path.join(data_dir, test_dir),\n",
    "                    data_transforms['test'])\n",
    "\n",
    "test_dataloader = DataLoader(test_images, batch_size=batch_size, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.AlexNet()\n",
    "model.classifier[6] = nn.Linear(4096, 10)\n",
    "model.load_state_dict(torch.load('alexnet_pretrained_erba.model', map_location=str(device)))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_show(tensors):\n",
    "\n",
    "    num_rows = 1\n",
    "    num_cols = len(labels)\n",
    "    fig = plt.figure(figsize=(num_cols, num_rows))\n",
    "    i = 0\n",
    "    for t in tensors:\n",
    "        t = np.transpose(t.cpu().numpy(), (1, 2, 0))\n",
    "        t = np.asarray(std).mean() * t + np.asarray(mean).mean()\n",
    "        t = np.clip(t, 0, 1)\n",
    "\n",
    "        ax1 = fig.add_subplot(num_rows, num_cols, i+1)\n",
    "        fig.set_size_inches(18.5, 10.5)\n",
    "        ax1.imshow(t, interpolation='none')\n",
    "        ax1.axis('off')\n",
    "        ax1.set_xticklabels([])\n",
    "        ax1.set_yticklabels([])\n",
    "        ax1.set_title('Default')\n",
    "        i += 1\n",
    "\n",
    "    plt.subplots_adjust(wspace=0, hspace=0.1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_show(img):\n",
    "    img = np.asarray(std).mean() * img + np.asarray(mean).mean()\n",
    "    img = np.clip(img, 0, 1)\n",
    "    plt.imshow(img)\n",
    "    plt.pause(0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Buffered:\n",
    "    def __init__(self):\n",
    "        self.buffer_normal_image = []\n",
    "        self.buffer_heatmap_image = []\n",
    "        self.buffer_label = []        \n",
    "        \n",
    "    def show_buffered_img(self, normal, heatmap, label):\n",
    "        self.buffer_heatmap_image.append(heatmap)\n",
    "        print(len(self.buffer_heatmap_image))\n",
    "        self.buffer_label.append(label)\n",
    "        if len(self.buffer_heatmap_image) > 7:\n",
    "            fig = plt.figure(figsize=(batch_size, 1))\n",
    "            for i in range(len(self.buffer_heatmap_image)):\n",
    "                ax1 = fig.add_subplot(1, batch_size, i+1)\n",
    "                fig.set_size_inches(18.5, 10.5)\n",
    "                ax1.imshow(self.buffer_heatmap_image[i], interpolation='none')\n",
    "                ax1.axis('off')\n",
    "                ax1.set_xticklabels([])\n",
    "                ax1.set_yticklabels([])\n",
    "                ax1.set_title(self.buffer_label[i])\n",
    "            self.buffer_normal_image = []\n",
    "            self.buffer_heatmap_image = []\n",
    "            self.buffer_label = []\n",
    "            plt.subplots_adjust(wspace=0, hspace=0.1)\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class CamExtractor():\n",
    "    \"\"\"\n",
    "        Extracts cam features from the model\n",
    "    \"\"\"\n",
    "    def __init__(self, model, target_layer):\n",
    "        self.model = model\n",
    "        self.target_layer = target_layer\n",
    "        self.gradients = None\n",
    "\n",
    "    def save_gradient(self, grad):\n",
    "        self.gradients = grad\n",
    "\n",
    "    def forward_pass_on_convolutions(self, x):\n",
    "        \"\"\"\n",
    "            Does a forward pass on convolutions, hooks the function at given layer\n",
    "        \"\"\"\n",
    "        conv_output = None\n",
    "        for module_pos, module in self.model.features._modules.items():\n",
    "            x = module(x)  # Forward\n",
    "            if int(module_pos) == self.target_layer:\n",
    "                x.register_hook(self.save_gradient)\n",
    "                conv_output = x  # Save the convolution output on that layer\n",
    "        return conv_output, x\n",
    "\n",
    "    def forward_pass(self, x):\n",
    "        \"\"\"\n",
    "            Does a full forward pass on the model\n",
    "        \"\"\"\n",
    "        # Forward pass on the convolutions\n",
    "        conv_output, x = self.forward_pass_on_convolutions(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        # Forward pass on the classifier\n",
    "        x = self.model.classifier(x)\n",
    "        return conv_output, x\n",
    "\n",
    "\n",
    "class GradCam():\n",
    "    \"\"\"\n",
    "        Produces class activation map\n",
    "    \"\"\"\n",
    "    def __init__(self, model, target_layer):\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "        # Define extractor\n",
    "        self.extractor = CamExtractor(self.model, target_layer)\n",
    "\n",
    "    def generate_cam(self, input_image, target_class=None):\n",
    "\n",
    "        # Full forward pass\n",
    "        # conv_output is the output of convolutions at specified layer\n",
    "        # model_output is the final output of the model (1, 1000)\n",
    "        \n",
    "        # forward completo, viene salvato il gradiente del target layer,\n",
    "        # conv_out è l'uscita dal convolution target layer\n",
    "        # model_output è l'uscita dall'ultimo layer della rete\n",
    "        conv_output, model_output = self.extractor.forward_pass(input_image)\n",
    "        if target_class is None:\n",
    "            target_class = np.argmax(model_output.data.numpy())\n",
    "\n",
    "        # Target for backprop\n",
    "        \n",
    "        # Inizializzazione di one_hot_output come tensore tutto di 0 e in posizione [0][target_class]\n",
    "        # assegna 1\n",
    "        one_hot_output = torch.FloatTensor(1, model_output.size()[-1]).zero_()\n",
    "        one_hot_output[0][target_class] = 1\n",
    "\n",
    "        # Zero grads\n",
    "\n",
    "        # reinizializzazione del gradiente\n",
    "        # quando si fa backprogation il gradiente va reinizializzato se no le modifiche che verranno\n",
    "        # apportate nella backpropagation andrebbero a sommarsi al gradiente calcolato precedentemente durante\n",
    "        # il forward\n",
    "        self.model.features.zero_grad()\n",
    "        self.model.classifier.zero_grad()\n",
    "\n",
    "        # Backward pass with specified target\t\n",
    "        model_output.backward(gradient=one_hot_output, retain_graph=True)\n",
    "        # Get hooked gradients\n",
    "        guided_gradients = self.extractor.gradients.data.numpy()[0]\n",
    "        # Get convolution outputs\n",
    "        target = conv_output.data.numpy()[0]\n",
    "\n",
    "        # Get weights from gradients\n",
    "\n",
    "        # I gradienti sono 1 x ogni neurone quindi viene eseguita una media\n",
    "        weights = np.mean(guided_gradients, axis=(1, 2))  # Take averages for each gradient\n",
    "        # Create empty numpy array for cam\n",
    "        cam = np.ones(target.shape[1:], dtype=np.float32)\n",
    "        # Multiply each weight with its conv output and then, sum\n",
    "        for i, w in enumerate(weights):\n",
    "            cam += w * target[i, :, :]\n",
    "        cam = cv2.resize(cam, (224, 224))\n",
    "        cam = np.maximum(cam, 0)\n",
    "        cam = (cam - np.min(cam)) / (np.max(cam) - np.min(cam))  # Normalize between 0-1\n",
    "        cam = np.uint8(cam * 255)  # Scale between 0-255 to visualize\n",
    "        return cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torchvision import models\n",
    "\n",
    "\n",
    "def convert_to_grayscale(cv2im):\n",
    "    \"\"\"\n",
    "        Converts 3d image to grayscale\n",
    "\n",
    "    Args:\n",
    "        cv2im (numpy arr): RGB image with shape (D,W,H)\n",
    "\n",
    "    returns:\n",
    "        grayscale_im (numpy_arr): Grayscale image with shape (1,W,D)\n",
    "    \"\"\"\n",
    "    grayscale_im = np.sum(np.abs(cv2im), axis=0)\n",
    "    im_max = np.percentile(grayscale_im, 99)\n",
    "    im_min = np.min(grayscale_im)\n",
    "    grayscale_im = (np.clip((grayscale_im - im_min) / (im_max - im_min), 0, 1))\n",
    "    grayscale_im = np.expand_dims(grayscale_im, axis=0)\n",
    "    return grayscale_im\n",
    "\n",
    "\n",
    "def save_gradient_images(gradient, file_name):\n",
    "    \"\"\"\n",
    "        Exports the original gradient image\n",
    "\n",
    "    Args:\n",
    "        gradient (np arr): Numpy array of the gradient with shape (3, 224, 224)\n",
    "        file_name (str): File name to be exported\n",
    "    \"\"\"\n",
    "    if not os.path.exists('../results'):\n",
    "        os.makedirs('../results')\n",
    "    gradient = gradient - gradient.min()\n",
    "    gradient /= gradient.max()\n",
    "    gradient = np.uint8(gradient * 255).transpose(1, 2, 0)\n",
    "    path_to_file = os.path.join('../results', file_name + '.jpg')\n",
    "    # Convert RBG to GBR\n",
    "    gradient = gradient[..., ::-1]\n",
    "    cv2.imwrite(path_to_file, gradient)\n",
    "\n",
    "\n",
    "def save_class_activation_on_image(org_img, activation_map, file_name):\n",
    "    \"\"\"\n",
    "        Saves cam activation map and activation map on the original image\n",
    "\n",
    "    Args:\n",
    "        org_img (PIL img): Original image\n",
    "        activation_map (numpy arr): activation map (grayscale) 0-255\n",
    "        file_name (str): File name of the exported image\n",
    "    \"\"\"\n",
    "    if not os.path.exists('../results'):\n",
    "        os.makedirs('../results')\n",
    "    # Grayscale activation map\n",
    "    path_to_file = os.path.join('../results', file_name+'_Cam_Grayscale.jpg')\n",
    "    cv2.imwrite(path_to_file, activation_map)\n",
    "    # Heatmap of activation map\n",
    "    activation_heatmap = cv2.applyColorMap(activation_map, cv2.COLORMAP_JET)\n",
    "    path_to_file = os.path.join('../results', file_name+'_Cam_Heatmap.jpg')\n",
    "    cv2.imwrite(path_to_file, activation_heatmap)\n",
    "    # Heatmap on picture\n",
    "    #org_img = cv2.resize(org_img, (224, 224))\n",
    "    img_with_heatmap = np.float32(activation_heatmap) + np.float32(org_img)\n",
    "    img_with_heatmap = img_with_heatmap / np.max(img_with_heatmap)\n",
    "    path_to_file = os.path.join('../results', file_name+'_Cam_On_Image.jpg')\n",
    "    cv2.imwrite(path_to_file, np.uint8(255 * img_with_heatmap))\n",
    "\n",
    "def show_class_activation_on_image(org_img, activation_map, buffered):\n",
    "    #plt.imshow(activation_map, cmap=\"gray\")\n",
    "    #plt.show()\n",
    "    activation_heatmap = cv2.applyColorMap(activation_map, cv2.COLORMAP_JET)\n",
    "    #image_show(activation_heatmap)\n",
    "    #img_with_heatmap = np.float32(activation_heatmap) + np.float32(org_img)\n",
    "    #img_with_heatmap = img_with_heatmap / np.max(img_with_heatmap)\n",
    "    #image_show(np.uint8(255 * img_with_heatmap))\n",
    "    \n",
    "    buffered.show_buffered_img(org_img, frame_extractor(activation_heatmap), \"label\")\n",
    "\n",
    "#def image_show(image):\n",
    "#    b,g,r = cv2.split(image)\n",
    "#    frame_rgb = cv2.merge((r,g,b))\n",
    "#    plt.imshow(frame_rgb)\n",
    "#    plt.show()\n",
    "\n",
    "def frame_extractor(image):\n",
    "    b,g,r = cv2.split(image)\n",
    "    return cv2.merge((r,g,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name_to_export = \"prova\"\n",
    "grad_cam = GradCam(model, target_layer=11)\n",
    "buffered = Buffered()\n",
    "model.train()\n",
    "for data in test_dataloader:\n",
    "    inputs, labels = data\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    for i in range(len(inputs)):\n",
    "        if(labels[i] == 2):\n",
    "            original = np.transpose(inputs[i].numpy(),(1,2,0))\n",
    "            single_input = inputs[i].unsqueeze_(0)\n",
    "            cam = grad_cam.generate_cam(single_input, labels[i])\n",
    "            print(classes[labels[i]])\n",
    "            #image_show(original)\n",
    "            show_class_activation_on_image(original, cam, buffered)\n",
    "            #save_class_activation_on_image(np.transpose(inputs[i].numpy(),(1,2,0)), cam, file_name_to_export)\n",
    "        \n",
    "        #cam = grad_cam.generate_cam(inputs, labels)\n",
    "        #save_class_activation_on_image(original_image, cam, file_name_to_export)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
