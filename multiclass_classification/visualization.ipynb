{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import ConcatDataset\n",
    "from torch.utils.data import TensorDataset\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchvision import models\n",
    "import shutil\n",
    "import os\n",
    "import math\n",
    "import copy\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "data_dir = '../data/'\n",
    "test_dir = 'normal_test'\n",
    "classes = sorted(os.listdir(data_dir + test_dir))\n",
    "batch_size = 16\n",
    "if os.path.exists(test_dir):\n",
    "    shutil.rmtree(test_dir)\n",
    "os.mkdir(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normal test normalization\n",
    "mean = [0.44947562, 0.46524084, 0.40037745]\n",
    "std = [0.18456618, 0.16353698, 0.20014246]\n",
    "\n",
    "# leaf normalization\n",
    "#mean = [0.360007843, 0.482983922, 0.274050667]\n",
    "#std = [0.177985098, 0.180084239, 0.141458866]\n",
    "\n",
    "data_transforms = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "        ])\n",
    "\n",
    "test_images = datasets.ImageFolder(os.path.join(data_dir, test_dir),\n",
    "                    data_transforms)\n",
    "\n",
    "test_dataloader = DataLoader(test_images, batch_size=batch_size, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data = []\\nfor images, labels in test_dataloader:\\n    for i in range(len(images)):\\n        numpy_image = images[i].numpy()\\n        image_mean = np.mean(numpy_image, axis=(1,2))\\n        std = np.std(numpy_image, axis=(1,2))\\n        adjusted_stddev = np.float32(np.maximum(std, [1.0/224.0, 1.0/224.0, 1.0/224.0]))\\n        image_mean_matrix = np.asarray([np.full((224,224), image_mean[0]), np.full((224,224), image_mean[1]), np.full((224,224), image_mean[2])])\\n        adjusted_stddev_matrix = np.asarray([np.full((224,224), adjusted_stddev[0]), np.full((224,224), adjusted_stddev[1]), np.full((224,224), adjusted_stddev[2])])\\n        images[i] = images[i].sub_(torch.from_numpy(image_mean_matrix))\\n        images[i] = images[i].div_(torch.from_numpy(adjusted_stddev_matrix))\\n            \\n    data.append(TensorDataset(images, labels))\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"data = []\n",
    "for images, labels in test_dataloader:\n",
    "    for i in range(len(images)):\n",
    "        numpy_image = images[i].numpy()\n",
    "        image_mean = np.mean(numpy_image, axis=(1,2))\n",
    "        std = np.std(numpy_image, axis=(1,2))\n",
    "        adjusted_stddev = np.float32(np.maximum(std, [1.0/224.0, 1.0/224.0, 1.0/224.0]))\n",
    "        image_mean_matrix = np.asarray([np.full((224,224), image_mean[0]), np.full((224,224), image_mean[1]), np.full((224,224), image_mean[2])])\n",
    "        adjusted_stddev_matrix = np.asarray([np.full((224,224), adjusted_stddev[0]), np.full((224,224), adjusted_stddev[1]), np.full((224,224), adjusted_stddev[2])])\n",
    "        images[i] = images[i].sub_(torch.from_numpy(image_mean_matrix))\n",
    "        images[i] = images[i].div_(torch.from_numpy(adjusted_stddev_matrix))\n",
    "            \n",
    "    data.append(TensorDataset(images, labels))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'datasets = []\\nfor d in data:\\n    datasets = ConcatDataset([datasets, d])\\n    \\ntest_dataloader = DataLoader(datasets, batch_size=batch_size, shuffle=False, num_workers=4)\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"datasets = []\n",
    "for d in data:\n",
    "    datasets = ConcatDataset([datasets, d])\n",
    "    \n",
    "test_dataloader = DataLoader(datasets, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlexNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "    (1): ReLU(inplace)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (4): ReLU(inplace)\n",
       "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace)\n",
       "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace)\n",
       "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace)\n",
       "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5)\n",
       "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "    (2): ReLU(inplace)\n",
       "    (3): Dropout(p=0.5)\n",
       "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (5): ReLU(inplace)\n",
       "    (6): Linear(in_features=4096, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.alexnet()\n",
    "model.classifier[6] = nn.Linear(4096, 10)\n",
    "model.load_state_dict(torch.load('alexnet_pretrained.model', map_location=str(device)))\n",
    "model.eval()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_show(tensors, labels, preds):\n",
    "\n",
    "    num_rows = 1\n",
    "    num_cols = len(labels)\n",
    "    fig = plt.figure(figsize=(num_rows, num_cols))\n",
    "    i = 0\n",
    "    for t in tensors:\n",
    "        t = np.transpose(t.cpu().numpy(), (1, 2, 0))\n",
    "        #t = np.asarray(std).mean() * t + np.asarray(mean).mean()\n",
    "        t = np.clip(t, 0, 1)\n",
    "    \n",
    "        ax1 = fig.add_subplot(1, num_cols, i+1)\n",
    "        fig.set_size_inches(18.5, 10.5)\n",
    "        ax1.imshow(t, interpolation='none')\n",
    "        ax1.axis('off')\n",
    "        if preds[i] != labels[i]:\n",
    "            ax1.set_title(classes[preds[i]][:10], size='x-small')\n",
    "        i += 1\n",
    "        \n",
    "    plt.subplots_adjust(wspace=0.1, hspace=0)\n",
    "    plt.savefig(test_dir + '/' + classes[labels[0]] + '_orig.jpg', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def occlusion(image, occluding_size, occluding_stride, model, classes, groundTruth):\n",
    "    img = np.copy(image)\n",
    "    height, width, _ = img.shape\n",
    "    output_height = int(math.ceil((height-occluding_size) / occluding_stride + 1))\n",
    "    output_width = int(math.ceil((width-occluding_size) / occluding_stride + 1))\n",
    "    occluded_images = []\n",
    "    for h in range(output_height):\n",
    "        for w in range(output_width):\n",
    "            # occluder region\n",
    "            h_start = h * occluding_stride\n",
    "            w_start = w * occluding_stride\n",
    "            h_end = min(height, h_start + occluding_size)\n",
    "            w_end = min(width, w_start + occluding_size)\n",
    "            \n",
    "            input_image = copy.copy(img)\n",
    "            input_image[h_start:h_end,w_start:w_end,:] = 0\n",
    "            occluded_images.append(transforms.ToTensor()(Image.fromarray(input_image)))\n",
    "            \n",
    "    L = np.empty(output_height * output_width)\n",
    "    L.fill(groundTruth)\n",
    "    L = torch.from_numpy(L)\n",
    "    tensor_images = torch.stack([img for img in occluded_images])\n",
    "    dataset = torch.utils.data.TensorDataset(tensor_images, L) \n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=128, shuffle=False, num_workers=8) \n",
    "\n",
    "    heatmap = np.empty(0)\n",
    "    for images, labels in dataloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        m = nn.Softmax(dim=1)\n",
    "        outputs = m(outputs)\n",
    "        outputs = outputs.cpu()\n",
    "        heatmap = np.concatenate((heatmap, outputs[0:outputs.size()[0], groundTruth].data.numpy()))\n",
    "        \n",
    "    return heatmap.reshape((output_height, output_width))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CamExtractor():\n",
    "    \"\"\"\n",
    "        Extracts cam features from the model\n",
    "    \"\"\"\n",
    "    def __init__(self, model, target_layer):\n",
    "        self.model = model\n",
    "        self.target_layer = target_layer\n",
    "        self.gradients = None\n",
    "\n",
    "    def save_gradient(self, grad):\n",
    "        self.gradients = grad\n",
    "\n",
    "    def forward_pass_on_convolutions(self, x):\n",
    "        \"\"\"\n",
    "            Does a forward pass on convolutions, hooks the function at given layer\n",
    "        \"\"\"\n",
    "        conv_output = None\n",
    "        for module_pos, module in self.model.features._modules.items():\n",
    "            x = module(x)  # Forward\n",
    "            if int(module_pos) == self.target_layer:\n",
    "                x.register_hook(self.save_gradient)\n",
    "                conv_output = x  # Save the convolution output on that layer\n",
    "        return conv_output, x\n",
    "\n",
    "    def forward_pass(self, x):\n",
    "        \"\"\"\n",
    "            Does a full forward pass on the model\n",
    "        \"\"\"\n",
    "        # Forward pass on the convolutions\n",
    "        conv_output, x = self.forward_pass_on_convolutions(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        # Forward pass on the classifier\n",
    "        x = self.model.classifier(x)\n",
    "        return conv_output, x\n",
    "\n",
    "\n",
    "class GradCam():\n",
    "    \"\"\"\n",
    "        Produces class activation map\n",
    "    \"\"\"\n",
    "    def __init__(self, model, target_layer):\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "        # Define extractor\n",
    "        self.extractor = CamExtractor(self.model, target_layer)\n",
    "\n",
    "    def generate_cam(self, input_image, target_class=None):\n",
    "\n",
    "        # Full forward pass\n",
    "        # conv_output is the output of convolutions at specified layer\n",
    "        # model_output is the final output of the model (1, 1000)\n",
    "        \n",
    "        # forward completo, viene salvato il gradiente del target layer,\n",
    "        # conv_out è l'uscita dal convolution target layer\n",
    "        # model_output è l'uscita dall'ultimo layer della rete\n",
    "        conv_output, model_output = self.extractor.forward_pass(input_image)\n",
    "        if target_class is None:\n",
    "            target_class = np.argmax(model_output.data.numpy())\n",
    "\n",
    "        # Target for backprop\n",
    "        \n",
    "        # Inizializzazione di one_hot_output come tensore tutto di 0 e in posizione [0][target_class]\n",
    "        # assegna 1\n",
    "        one_hot_output = torch.cuda.FloatTensor(1, model_output.size()[-1]).zero_()\n",
    "        one_hot_output[0][target_class] = 1\n",
    "\n",
    "        # Zero grads\n",
    "\n",
    "        # reinizializzazione del gradiente\n",
    "        # quando si fa backprogation il gradiente va reinizializzato se no le modifiche che verranno\n",
    "        # apportate nella backpropagation andrebbero a sommarsi al gradiente calcolato precedentemente durante\n",
    "        # il forward\n",
    "        self.model.features.zero_grad()\n",
    "        self.model.classifier.zero_grad()\n",
    "\n",
    "        # Backward pass with specified target\n",
    "        model_output.backward(gradient=one_hot_output, retain_graph=True)\n",
    "        # Get hooked gradients\n",
    "        guided_gradients = self.extractor.gradients.cpu().data.numpy()[0]\n",
    "        # Get convolution outputs\n",
    "        target = conv_output.cpu().data.numpy()[0]\n",
    "\n",
    "        # Get weights from gradients\n",
    "\n",
    "        # I gradienti sono 1 x ogni neurone quindi viene eseguita una media\n",
    "        weights = np.mean(guided_gradients, axis=(1, 2))  # Take averages for each gradient\n",
    "        # Create empty numpy array for cam\n",
    "        cam = np.ones(target.shape[1:], dtype=np.float32)\n",
    "        # Multiply each weight with its conv output and then, sum\n",
    "        for i, w in enumerate(weights):\n",
    "            cam += w * target[i, :, :]\n",
    "        cam = cv2.resize(cam, (224, 224))\n",
    "        cam = np.maximum(cam, 0)\n",
    "        cam = (cam - np.min(cam)) / (np.max(cam) - np.min(cam))  # Normalize between 0-1\n",
    "        cam = np.uint8(cam * 255)  # Scale between 0-255 to visualize\n",
    "        return cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_class_activation_on_image(activation_map):\n",
    "    #plt.imshow(activation_map, cmap=\"gray\")\n",
    "    #plt.show()\n",
    "    activation_heatmap = cv2.applyColorMap(activation_map, cv2.COLORMAP_JET)\n",
    "    #image_show(activation_heatmap)\n",
    "    #img_with_heatmap = np.float32(activation_heatmap) + np.float32(org_img)\n",
    "    #img_with_heatmap = img_with_heatmap / np.max(img_with_heatmap)\n",
    "    #image_show(np.uint8(255 * img_with_heatmap))\n",
    "    \n",
    "    return frame_extractor(activation_heatmap)\n",
    "\n",
    "def frame_extractor(image):\n",
    "    b, g, r = cv2.split(image)\n",
    "    return cv2.merge((r, g, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_cam = GradCam(model, target_layer=11)\n",
    "patch_size = 80\n",
    "patch_stride = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se le immagini sono inferiori a dim_batch => rivedere\n",
    "for cls in range(len(classes)):\n",
    "    for inputs, labels in test_dataloader:\n",
    "        if labels[0] != cls:\n",
    "            continue\n",
    "        \n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        \n",
    "        image_show(inputs, labels, preds)\n",
    "        \"\"\"\n",
    "        cams = []\n",
    "        for i in range(len(labels)):\n",
    "            cam = grad_cam.generate_cam(inputs[i].unsqueeze_(0), labels[i])\n",
    "            cams.append(show_class_activation_on_image(cam))\n",
    "\n",
    "        fig = plt.figure(figsize=(len(labels), 1))\n",
    "        for i in range(len(cams)):\n",
    "            ax1 = fig.add_subplot(1, len(labels), i+1)\n",
    "            ax1.axis('off')\n",
    "            fig.set_size_inches(18.5, 10.5)\n",
    "            ax1.imshow(cams[i], interpolation='none')\n",
    "\n",
    "        plt.subplots_adjust(wspace=0.1, hspace=0)\n",
    "        plt.savefig(test_dir + '/' + classes[labels[0]] + '_cam.jpg', bbox_inches='tight')\n",
    "        \n",
    "        occludeds = []\n",
    "        for i in range(len(labels)):\n",
    "            ind = labels[i]\n",
    "            img = transforms.ToPILImage()(inputs[i].cpu())\n",
    "            occludeds.append(occlusion(img, patch_size, patch_stride, model, classes, ind))\n",
    "\n",
    "        fig = plt.figure(figsize=(len(labels), 1))\n",
    "        for i in range(len(occludeds)):\n",
    "            ax1 = fig.add_subplot(1, len(labels), i+1)\n",
    "            ax1.axis('off')\n",
    "            fig.set_size_inches(18.5, 10.5)\n",
    "            sns.heatmap(occludeds[i], cmap='jet_r', cbar=False, square=True, ax=ax1)\n",
    "\n",
    "        plt.subplots_adjust(wspace=0.1, hspace=0)\n",
    "        plt.savefig(test_dir + '/' + classes[labels[0]] + '_occ.jpg', bbox_inches='tight')\n",
    "        \"\"\"\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
